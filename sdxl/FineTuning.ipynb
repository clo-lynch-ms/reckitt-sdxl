{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning on an SDXL model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/diffusers\n",
      "  Cloning https://github.com/huggingface/diffusers to /tmp/pip-req-build-tig0zxfz\n",
      "  Running command git clone -q https://github.com/huggingface/diffusers /tmp/pip-req-build-tig0zxfz\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from diffusers==0.20.0.dev0) (2.22.0)\n",
      "Requirement already satisfied: Pillow in /home/azureuser/.local/lib/python3.8/site-packages (from diffusers==0.20.0.dev0) (10.0.0)\n",
      "Requirement already satisfied: numpy in /home/azureuser/.local/lib/python3.8/site-packages (from diffusers==0.20.0.dev0) (1.24.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/azureuser/.local/lib/python3.8/site-packages (from diffusers==0.20.0.dev0) (0.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/azureuser/.local/lib/python3.8/site-packages (from diffusers==0.20.0.dev0) (2023.6.3)\n",
      "Requirement already satisfied: filelock in /home/azureuser/.local/lib/python3.8/site-packages (from diffusers==0.20.0.dev0) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.2 in /home/azureuser/.local/lib/python3.8/site-packages (from diffusers==0.20.0.dev0) (0.16.4)\n",
      "Requirement already satisfied: importlib-metadata in /home/azureuser/.local/lib/python3.8/site-packages (from diffusers==0.20.0.dev0) (6.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.20.0.dev0) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/azureuser/.local/lib/python3.8/site-packages (from huggingface-hub>=0.13.2->diffusers==0.20.0.dev0) (4.7.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/azureuser/.local/lib/python3.8/site-packages (from huggingface-hub>=0.13.2->diffusers==0.20.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/azureuser/.local/lib/python3.8/site-packages (from huggingface-hub>=0.13.2->diffusers==0.20.0.dev0) (23.1)\n",
      "Requirement already satisfied: fsspec in /home/azureuser/.local/lib/python3.8/site-packages (from huggingface-hub>=0.13.2->diffusers==0.20.0.dev0) (2023.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata->diffusers==0.20.0.dev0) (1.0.0)\n",
      "Building wheels for collected packages: diffusers\n",
      "  Building wheel for diffusers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for diffusers: filename=diffusers-0.20.0.dev0-py3-none-any.whl size=1321054 sha256=86520a22e1976dd816bbe62631a30d425f3f6ac4693d569abb71875314566126\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z3i3r78r/wheels/c6/77/b7/6d22ce35b79fbe5cc7513554d61c918bb4bf3eac5fdb8ae787\n",
      "Successfully built diffusers\n",
      "Installing collected packages: diffusers\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.19.3\n",
      "    Uninstalling diffusers-0.19.3:\n",
      "      Successfully uninstalled diffusers-0.19.3\n",
      "Successfully installed diffusers-0.20.0.dev0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade diffusers[torch]\n",
    "%pip install git+https://github.com/huggingface/diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate>=0.16.0 torchvision transformers>=4.25.1 ftfy tensorboard Jinja2 invisible-watermark>=0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: huggingface_hub[cli] in /home/azureuser/.local/lib/python3.8/site-packages (0.16.4)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.9 in /home/azureuser/.local/lib/python3.8/site-packages (from huggingface_hub[cli]) (23.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.42.1 in /home/azureuser/.local/lib/python3.8/site-packages (from huggingface_hub[cli]) (4.65.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /home/azureuser/.local/lib/python3.8/site-packages (from huggingface_hub[cli]) (3.12.2)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface_hub[cli]) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in /home/azureuser/.local/lib/python3.8/site-packages (from huggingface_hub[cli]) (4.7.1)\n",
      "Requirement already satisfied, skipping upgrade: fsspec in /home/azureuser/.local/lib/python3.8/site-packages (from huggingface_hub[cli]) (2023.6.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/lib/python3/dist-packages (from huggingface_hub[cli]) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: InquirerPy==0.3.4; extra == \"cli\" in /home/azureuser/.local/lib/python3.8/site-packages (from huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied, skipping upgrade: pfzy<0.4.0,>=0.3.1 in /home/azureuser/.local/lib/python3.8/site-packages (from InquirerPy==0.3.4; extra == \"cli\"->huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied, skipping upgrade: prompt-toolkit<4.0.0,>=3.0.1 in /home/azureuser/.local/lib/python3.8/site-packages (from InquirerPy==0.3.4; extra == \"cli\"->huggingface_hub[cli]) (3.0.39)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /home/azureuser/.local/lib/python3.8/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4; extra == \"cli\"->huggingface_hub[cli]) (0.2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade huggingface_hub[cli]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /home/azureuser/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clear cuda memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resizing training images to overcome cuda memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imagesize\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: imagesize\n",
      "Successfully installed imagesize-1.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imagesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagesize\n",
    "\n",
    "def check_dims(img_path):\n",
    "    width, height = imagesize.get(img_path)\n",
    "    print(width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_dims(\"content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_inside_of_0ee13733-787f-4700-a760-1013e63c0eb7.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(img_path):\n",
    "    # basewidth = 300\n",
    "    # img = Image.open(img_path)\n",
    "    # wpercent = (basewidth/float(img.size[0]))\n",
    "    # hsize = int((float(img.size[1])*float(wpercent)))\n",
    "    # img = img.resize((basewidth,hsize), Image.Resampling.LANCZOS)\n",
    "    # img.save(img_path)\n",
    "    with Image.open(img_path) as img:\n",
    "       img.resize((300,300))\n",
    "       img.save(img_path, dpi=(300,300))\n",
    "       print(str(img_path) + \":     done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize_img(\"content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_inside_of_0ee13733-787f-4700-a760-1013e63c0eb7.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_dims(\"content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_inside_of_0ee13733-787f-4700-a760-1013e63c0eb7.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_intricate_cf441f16-9f87-4cab-94d3-bdec382d2c4e.png', 'content/data/dishwasher/olagajkowska_Generate_a_highly_detailed_and_realistic_close-up__1ee952a4-4968-47c9-8c15-3ac3b8188b66.png', 'content/data/dishwasher/olagajkowska_Generate_a_highly_realistic_and_detailed_image_of__27e6e82b-a06b-4cb7-bddd-7338c5e974b3.png', 'content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_inside_of_c4eebfa4-aac9-4185-93bd-dda701fd93cb.png', 'content/data/dishwasher/olagajkowska_Generate_a_highly_detailed_and_realistic_close-up__1cdfe67f-2f53-48be-8bb8-2bc9d177d33c.png', 'content/data/dishwasher/olagajkowska_Generate_a_highly_realistic_and_detailed_image_of__71ea58d8-1807-4791-80c1-c0c1b3063c79.png', 'content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_inside_of_80366929-1d19-45d4-bd74-333640231da5.png', 'content/data/dishwasher/olagajkowska_Generate_a_highly_detailed_and_realistic_close-up__5013ed35-7980-492b-b303-f7f3d5f697aa.png', 'content/data/dishwasher/olagajkowska_Generate_a_highly_realistic_and_detailed_image_of__3a9857f7-8ad3-4a70-abc2-e25cb4365793.png', 'content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_inside_of_0ee13733-787f-4700-a760-1013e63c0eb7.png', 'content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_inside_of_90fac45b-8ef0-4f41-a609-44589d51dbe8.png', 'content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_inside_of_1a5f648e-48d3-4beb-aac9-c8b15328b616.png', 'content/data/dishwasher/olagajkowska_Generate_a_highly_detailed_and_realistic_close-up__e345c44b-9d08-4bf7-9f65-dd08102b58fc.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "new_names = []\n",
    "for dirpath, dirnames, filenames in os.walk('content/data'):\n",
    "    for filename in filenames:\n",
    "        new_names.append(os.path.join(dirpath, filename))\n",
    "\n",
    "print(new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 300\n",
      "content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_intricate_cf441f16-9f87-4cab-94d3-bdec382d2c4e.png:     done\n",
      "300 300\n",
      "300 300\n",
      "content/data/dishwasher/olagajkowska_Generate_a_highly_detailed_and_realistic_close-up__1ee952a4-4968-47c9-8c15-3ac3b8188b66.png:     done\n",
      "300 300\n",
      "300 300\n",
      "content/data/dishwasher/olagajkowska_Generate_a_highly_realistic_and_detailed_image_of__27e6e82b-a06b-4cb7-bddd-7338c5e974b3.png:     done\n",
      "300 300\n",
      "300 300\n",
      "content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_inside_of_c4eebfa4-aac9-4185-93bd-dda701fd93cb.png:     done\n",
      "300 300\n",
      "300 300\n",
      "content/data/dishwasher/olagajkowska_Generate_a_highly_detailed_and_realistic_close-up__1cdfe67f-2f53-48be-8bb8-2bc9d177d33c.png:     done\n",
      "300 300\n",
      "300 300\n",
      "content/data/dishwasher/olagajkowska_Generate_a_highly_realistic_and_detailed_image_of__71ea58d8-1807-4791-80c1-c0c1b3063c79.png:     done\n",
      "300 300\n",
      "300 300\n",
      "content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_inside_of_80366929-1d19-45d4-bd74-333640231da5.png:     done\n",
      "300 300\n",
      "300 300\n",
      "content/data/dishwasher/olagajkowska_Generate_a_highly_detailed_and_realistic_close-up__5013ed35-7980-492b-b303-f7f3d5f697aa.png:     done\n",
      "300 300\n",
      "300 300\n",
      "content/data/dishwasher/olagajkowska_Generate_a_highly_realistic_and_detailed_image_of__3a9857f7-8ad3-4a70-abc2-e25cb4365793.png:     done\n",
      "300 300\n",
      "300 300\n",
      "content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_inside_of_0ee13733-787f-4700-a760-1013e63c0eb7.png:     done\n",
      "300 300\n",
      "300 300\n",
      "content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_inside_of_90fac45b-8ef0-4f41-a609-44589d51dbe8.png:     done\n",
      "300 300\n",
      "300 300\n",
      "content/data/dishwasher/olagajkowska_Generate_a_close-up_image_showcasing_the_inside_of_1a5f648e-48d3-4beb-aac9-c8b15328b616.png:     done\n",
      "300 300\n",
      "300 300\n",
      "content/data/dishwasher/olagajkowska_Generate_a_highly_detailed_and_realistic_close-up__e345c44b-9d08-4bf7-9f65-dd08102b58fc.png:     done\n",
      "300 300\n"
     ]
    }
   ],
   "source": [
    "for img_path in new_names:\n",
    "    check_dims(img_path)\n",
    "    resize_img(img_path)\n",
    "    check_dims(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla V100-PCIE-16GB (UUID: GPU-d7b78fb0-a468-5050-d62f-ef83e57e1cd9)\n",
      "GPU 1: Tesla V100-PCIE-16GB (UUID: GPU-ae16006b-c1fe-f38c-6035-0816290b111e)\n",
      "GPU 2: Tesla V100-PCIE-16GB (UUID: GPU-f261e378-b67c-b58f-aa32-436df6945fa5)\n",
      "GPU 3: Tesla V100-PCIE-16GB (UUID: GPU-b9f6b2b3-5db8-67a0-c6bd-ef131f39398b)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 31 14:37:13 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   30C    P0    37W / 250W |  13837MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  On   | 00000002:00:00.0 Off |                  Off |\n",
      "| N/A   28C    P0    24W / 250W |      4MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-PCIE...  On   | 00000003:00:00.0 Off |                  Off |\n",
      "| N/A   28C    P0    24W / 250W |      4MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-PCIE...  On   | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    26W / 250W |      4MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     30969      C   /bin/python3                    13833MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'native'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# # os.getenv['PYTORCH_CUDA_ALLOC_CONF']\n",
    "# torch.cuda.get_allocator_backend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train a fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_XL=\"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "INSTANCE_DIR=\"content/data/dishwasher\"\n",
    "OUTPUT_DIR=\"content/sdxl_weights/dishwasher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/31/2023 11:29:21 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 3\n",
      "Local process index: 3\n",
      "Device: cuda:3\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "07/31/2023 11:29:21 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "07/31/2023 11:29:21 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 1\n",
      "Local process index: 1\n",
      "Device: cuda:1\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "07/31/2023 11:29:21 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 2\n",
      "Local process index: 2\n",
      "Device: cuda:2\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'clip_sample_range', 'variance_type', 'thresholding', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "07/31/2023 11:29:40 - INFO - __main__ - ***** Running training *****\n",
      "07/31/2023 11:29:40 - INFO - __main__ -   Num examples = 13\n",
      "07/31/2023 11:29:40 - INFO - __main__ -   Num batches each epoch = 4\n",
      "07/31/2023 11:29:40 - INFO - __main__ -   Num Epochs = 500\n",
      "07/31/2023 11:29:40 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "07/31/2023 11:29:40 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "07/31/2023 11:29:40 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "07/31/2023 11:29:40 - INFO - __main__ -   Total optimization steps = 500\n",
      "Steps:   0%|          | 1/500 [00:08<1:14:36,  8.97s/it, loss=0.0062, lr=0.0001]07/31/2023 11:29:49 - INFO - torch.nn.parallel.distributed - Reducer buckets have been rebuilt in this iteration.\n",
      "07/31/2023 11:29:49 - INFO - torch.nn.parallel.distributed - Reducer buckets have been rebuilt in this iteration.\n",
      "07/31/2023 11:29:49 - INFO - torch.nn.parallel.distributed - Reducer buckets have been rebuilt in this iteration.\n",
      "07/31/2023 11:29:49 - INFO - torch.nn.parallel.distributed - Reducer buckets have been rebuilt in this iteration.\n",
      "Steps: 100%|███████████| 500/500 [57:38<00:00,  6.90s/it, loss=0.247, lr=0.0001]07/31/2023 12:27:18 - INFO - accelerate.accelerator - Saving current state to content/sdxl_weights/dishwasher/checkpoint-500\n",
      "Traceback (most recent call last):\n",
      "  File \"train_dreambooth_lora_sdxl.py\", line 1355, in <module>\n",
      "    main(args)\n",
      "  File \"train_dreambooth_lora_sdxl.py\", line 1178, in main\n",
      "    accelerator.save_state(save_path)\n",
      "  File \"/home/azureuser/.local/lib/python3.8/site-packages/accelerate/accelerator.py\", line 2551, in save_state\n",
      "    weights.append(self.get_state_dict(model, unwrap=False))\n",
      "  File \"/home/azureuser/.local/lib/python3.8/site-packages/accelerate/accelerator.py\", line 2851, in get_state_dict\n",
      "    state_dict[k] = state_dict[k].float()\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 15.78 GiB total capacity; 13.89 GiB already allocated; 73.50 MiB free; 14.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Steps: 100%|███████████| 500/500 [57:39<00:00,  6.92s/it, loss=0.247, lr=0.0001]\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31053 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31054 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31055 closing signal SIGTERM\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 31052) of binary: /bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/azureuser/.local/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/azureuser/.local/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
      "    args.func(args)\n",
      "  File \"/home/azureuser/.local/lib/python3.8/site-packages/accelerate/commands/launch.py\", line 970, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/home/azureuser/.local/lib/python3.8/site-packages/accelerate/commands/launch.py\", line 646, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/home/azureuser/.local/lib/python3.8/site-packages/torch/distributed/run.py\", line 785, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/azureuser/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/azureuser/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "train_dreambooth_lora_sdxl.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2023-07-31_12:27:21\n",
      "  host      : localhost\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 31052)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train_dreambooth_lora_sdxl.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME_XL  \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --instance_prompt=\"a photo of a dishwasher mid-cycle\" \\\n",
    "  --resolution=1024 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=500 \\\n",
    "  --seed=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_DIR_XL = \"\"\n",
    "if WEIGHTS_DIR_XL == \"\":\n",
    "    from natsort import natsorted\n",
    "    from glob import glob\n",
    "    import os\n",
    "    WEIGHTS_DIR_XL = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
    "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR_XL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference using fine tuned sdxl model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## instllations\n",
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler #TODO: DiffusionPipeline\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = WEIGHTS_DIR_XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_xl_clean_dishes = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\") #TODO: DiffusionPipeline\n",
    "pipe_xl_clean_dishes.scheduler = DDIMScheduler.from_config(pipe_xl_clean_dishes.scheduler.config)\n",
    "pipe_xl_clean_dishes.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_cuda = torch.Generator(device='cuda')\n",
    "seed = 52362\n",
    "g_cuda.manual_seed(seed)\n",
    "# g_cuda = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"photo of clean dishes, fine grained, clear, sparkling dishes\" # instance_prompt = \"photo of a clean dishwasher\"\n",
    "negative_prompt = \"\"\n",
    "num_samples = 4 \n",
    "guidance_scale = 7.5 \n",
    "num_inference_steps = 24 \n",
    "height = 512 \n",
    "width = 512 \n",
    "\n",
    "with autocast(\"cuda\"), torch.inference_mode():\n",
    "    images = pipe_xl_clean_dishes(\n",
    "        prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=num_samples,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=g_cuda\n",
    "    ).images\n",
    "\n",
    "for img in images:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refiner\n",
    "https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
